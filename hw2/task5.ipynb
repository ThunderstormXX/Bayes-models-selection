{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import statsmodels.stats.multitest as smt\n",
    "from scipy.stats import t\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sampling = 10000\n",
    "m = 1000\n",
    "n = 10\n",
    "k = 4\n",
    "sigma = 1\n",
    "value_lambda = np.zeros(n)\n",
    "principal_component = np.zeros(n)\n",
    "for _ in range(N_sampling) :\n",
    "    x_1 = np.random.normal(0, sigma, m)\n",
    "    x_2 = np.random.normal(0, sigma, m)\n",
    "    X = []\n",
    "    for j in range(n-1):\n",
    "        eps = np.random.normal(0, sigma, m)\n",
    "        X.append(x_1 + eps)\n",
    "    X.append(k*x_2)\n",
    "    X = np.array(X)\n",
    "    C = 1/m*X@X.T\n",
    "\n",
    "    U, s, VT = np.linalg.svd(C)\n",
    "    value_lambda += s\n",
    "    if np.max(U[:,0]) > 0 :\n",
    "        principal_component += U[:,0] \n",
    "    else :\n",
    "        principal_component -= U[:,0]\n",
    "E_lambda = value_lambda / N_sampling\n",
    "princ_comp = principal_component/N_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.04640714  9.97856899  1.14694204  1.09329167  1.05100433  1.01295466\n",
      "  0.97682351  0.94067172  0.90278869  0.85726817]\n"
     ]
    }
   ],
   "source": [
    "print(E_lambda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим на первом месте стоит 16 потому , что это k**2 который больше n ,а на втором месте 10 = n, остальные $\\lambda$ = 1 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Главная компонента : (Видим , что это прмерно базисный вектор умноженный на константу) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0041566  -0.00184678 -0.00194047 -0.00181231 -0.00181544 -0.00190821\n",
      " -0.00179753 -0.00177232 -0.00164267  0.58693099]\n"
     ]
    }
   ],
   "source": [
    "print(princ_comp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случай K**2<n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sampling = 10000\n",
    "m = 100\n",
    "n = 10\n",
    "k = 2\n",
    "sigma = 1\n",
    "value_lambda = np.zeros(n)\n",
    "principal_component = np.zeros(n)\n",
    "for _ in range(N_sampling) :\n",
    "    x_1 = np.random.normal(0, sigma, m)\n",
    "    x_2 = np.random.normal(0, sigma, m)\n",
    "    X = []\n",
    "    for j in range(n-1):\n",
    "        eps = np.random.normal(0, sigma, m)\n",
    "        X.append(x_1 + eps)\n",
    "    X.append(k*x_2)\n",
    "    X = np.array(X)\n",
    "    C = 1/m*X@X.T\n",
    "\n",
    "    U, s, VT = np.linalg.svd(C)\n",
    "    value_lambda += s\n",
    "    if np.max(np.abs(U[:,0])) > 0 :\n",
    "        principal_component += U[:,0] \n",
    "    else :\n",
    "        principal_component -= U[:,0]\n",
    "E_lambda = value_lambda / N_sampling\n",
    "princ_comp = principal_component/N_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.14209408  4.0363755   1.46977018  1.27467537  1.12886844  1.00389718\n",
      "  0.89226275  0.78715596  0.68273645  0.56806693]\n"
     ]
    }
   ],
   "source": [
    "print(E_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.32952354 -0.32942955 -0.32990661 -0.32935573 -0.33012941 -0.32947379\n",
      " -0.32977466 -0.3300324  -0.32958409 -0.00044196]\n"
     ]
    }
   ],
   "source": [
    "print(princ_comp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим результаты соответствуют нашей теории : теперь на первом месте стоит $\\lambda$ = 10 = n, а главная компонента это вектор из единичек и в конце ноль умноженный на константу"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Хоть результаты и соответствуют теории, но есть некотрый вчислительный эффект от умножения матриц из-за которого даже при сэмплировании , собственные значения заметно варьируются вокруг их реального значения\n",
    "2) При разложении SVD матрицы ковариаций прзнаков : большие значения собственных чисел не обязательно говорят о том ,что данные признаки самые существенные, а остальные  нет. Просто этот признак мог входить в каждый другой признак .В нашем случае после линейного преобращования можно прийти к n информативным признакам , которые выбрасывать нельзя (если мы интерпретируем eps_i не как шум ,а как информативные признаки ). Так мы могли бы перейти в признаковое пространство из 2 признаков, но потеряли бы информацию от всех остальных признаков ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
